# Домашнее задание к занятию "`Репликация и масштабирование`" - `Евгений Головенко`

---

### Задание 1

Выполните конфигурацию master-slave репликации, примером можно пользоваться из лекции.

*Приложите скриншоты конфигурации, выполнения работы: состояния и режимы работы серверов.*

### Решение 1

Конфигурация Master:

`dockerfile_master`
``` docker
FROM mysql:8.0
COPY ./master.cnf /etc/mysql/conf.d/my.cnf
COPY ./master.sql /docker-entrypoint-initdb.d/master.sql
ENV MYSQL_ROOT_PASSWORD=rootpass
CMD ["mysqld"]
```

`master.cnf`
```
[mysqld]
server-id = 1
log-bin = mysql-bin
binlog_format=ROW
```
`master.sql`
```
CREATE USER 'repl'@'%' IDENTIFIED BY 'slavepass';
GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';
FLUSH PRIVILEGES;
```
Конфигурация Slave:

`dockerfile_slave`
```
FROM mysql:8.0
COPY ./slave.cnf /etc/mysql/conf.d/my.cnf
COPY ./slave.sql /docker-entrypoint-initdb.d/start.sql
ENV MYSQL_ROOT_PASSWORD=rootpass
CMD ["mysqld"]
```
`slave.cnf`
```
[mysqld]
server-id = 2
read-only = 1
super-read-only = 0
relay-log = relay-bin
```
`slave.sql`
```
CHANGE REPLICATION SOURCE TO
SOURCE_HOST='mysql_master',
SOURCE_USER='repl',
SOURCE_PASSWORD='slavepass',
SOURCE_SSL=1;
START REPLICA;
```

Старт контейнеров:

<img width="1082" height="602" alt="Conts" src="https://github.com/user-attachments/assets/b13f0c75-591e-47f1-b861-0cee8e209aad" />

Проверка сети:
```
PS E:\IT_Courses\Netologia\Data_Bases\12-06\Repl> docker network inspect replication
[
    {
        "Name": "replication",
        "Id": "42b7c053f9f94442469785d4b389fe7b9936491d3d4008568cb1b24d372456a8",
        "Created": "2026-01-20T18:42:18.538218991Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv4": true,
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "172.18.0.0/16",
                    "IPRange": "",
                    "Gateway": "172.18.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Options": {
            "com.docker.network.enable_ipv4": "true",
            "com.docker.network.enable_ipv6": "false"
        },
            "com.docker.network.enable_ipv6": "false"
        },
            "com.docker.network.enable_ipv6": "false"
        },
            "com.docker.network.enable_ipv6": "false"                                                 
        },
        "Labels": {},                                                                               
        "Containers": {
            "c5af6192f7cea4e6be1efb98a8f7f48a9684ed7871c4497da187b22ec67a02a6": {
                "Name": "mysql_slave",
                "EndpointID": "17085203bf2dabddd3f3ade3cc84de81c706f89c6f82abb0f0691082f000773e",  
                "MacAddress": "36:92:6c:fe:61:18",
                "IPv4Address": "172.18.0.3/16",                                                       
                "IPv6Address": ""
            },                                                                                      
            "ed1d9ee10e0314448b1534510f27984e1456d1099d898dc7880883d7e33b57e1": {
                "Name": "mysql_master",
                "EndpointID": "710d550aaafa087cc92c1b58070722b6dcb10eb80424d387e31ea2f2123e02a7",  
                "MacAddress": "7e:35:e2:1f:ab:8f",
                "IPv4Address": "172.18.0.2/16",
                "IPv6Address": ""
            }
        },
        "Status": {
            "IPAM": {
                "Subnets": {
                    "172.18.0.0/16": {
                        "IPsInUse": 5,
                        "DynamicIPsAvailable": 65531
                    }
                }
            }
        }
    }
]
```

Состояния и режимы работы:

<img width="899" height="590" alt="master_status" src="https://github.com/user-attachments/assets/2faf234e-8bbd-4775-a5ff-c43d563f5f3e" />

<img width="1402" height="588" alt="slave_status" src="https://github.com/user-attachments/assets/ca3d6e87-80c2-4b6d-8830-b19ae566f8d8" />

Проверка работы репликации:

<img width="783" height="594" alt="master_rec" src="https://github.com/user-attachments/assets/40886e52-5d33-4fe2-b15d-4bc78ed6a242" />

<img width="771" height="589" alt="slave_rep" src="https://github.com/user-attachments/assets/1d7e59a2-09e0-4bda-b3a6-2225ae8f86bb" />

---

### Задание 2

Разработайте план для выполнения горизонтального и вертикального шаринга базы данных. База данных состоит из трёх таблиц: 

- пользователи, 
- книги, 
- магазины (столбцы произвольно). 

Опишите принципы построения системы и их разграничение или разбивку между базами данных.

*Пришлите блоксхему, где и что будет располагаться. Опишите, в каких режимах будут работать сервера.* 

### Решение 2

Надо отметить, что постановка задачи несколько странная для курса "Системное администрирование" и начальные условия весьма туманны. Что за пользователи? Что мы о них знаем? Что известно о книгах? Где они? Какие и сколько? Сколько стоят? С магазинами вообще загадки во тьме. Чтож, попробую порассуждать.

Любое решение, в том числе и архитектурно-инфраструктурное, должно всегда отвечать на вопрос "Зачем мы это делаем?"

У сисадмина может быть прагматичный подход, не заморачиваясь на бизнес-логику, на возможности масштабирования...

Ок, это должно быть отказоустойчиво, с минимальными затратими времени и нервов на поддержку, с приемлемой нагрузкой на имеющиеся ресурсы (CPU, RAM, диски...). 
Здесь такая логика: база не помещается или/и тормозит. Значит надо разнести нагрузку по серверам, но так, чтобы это можно было как-то администрировать.
Самое простое, это сделать вертикальное разделение по таблицам. Минимум изменений и каждую базу можно обслуживать отдельно.

```
DB1 -> users
DB2 -> books
DB3 -> stores
```

У каждой DB свой сервер, свои бэкапы и реплики. Идеально!

Там, где много данных, чтож, будет горизонтальный шаринг. Ну, допустим

- users, быстро растет, значит делим;
- books, в принципе достаточно развесистая и склонна к росту, делим;
- stores, данных немного, перспективы роста неясны. Пусть так пока остается.

Как будем шарить?

Users - Про пользователей я уверено знаю только их ID. Вполне достаточно. Шарим по диапазонам ID.

```
users_1 -> id 1 - 1000
users_2 -> id 1001 - 2000
```
Приемлемо. Если прибегут еще - добавлю сервер.

Books. Аналогично.

```
books_1 → id 1 – 1 000 000
books_2 → id 1 000 001 – 2 000 000
```

Схема будет примерно такая:

```
              API
               |
    -----------|-----------
    |          |          |
 users DB   books DB   stores DB
    |          |
 users_1    books_1
 users_2    books_2
```

На каждую БД будет один основной сервер (пишем) 1 реплика (читаем), 1 бэкап.

Если мастер упал, переключаемся скриптом. 

С точки зрения сисадмина, это шедевр!

Да, но если не брать в голову JOIN между таблицами, запросы, изменения бизнес-логики и модели данных.

Но у владельца решения, а это скорее всего системный архитектор, может быть несколько отличный взгляд на задачу (но это не точно).
Допустим, как подопечная система будет жить дальше, как и куда будет развиваться? Что делать, чтоб она не упала? А если упала, тогда что?

Для решения этих вопросов было бы разумным подумать о том, как пользователи работают с данными, что их больше всего интересует (какие запросы самые частые), что будет при росте в несколько раз (шеф на прошлом митинге сказал, что через пару месяцев аналитики прогнозируют рост всего, что может расти. SMM радостно кивали, просмотры зашкаливают метрики моего холодильника в пятницу вечером), где должны быть границы ответственности причастных...

Т.е. подход сисадмина великолепен, но не учитывает бизнес-логику. После анализа бизнес-процессов возможно появиться подобный подход, что данные не должны лежать отдельно (пользователи отдельно, книги отдельно...), а вместе, в зависимости от того, как они совместно используются.

Например, пользователь и его данные, магазин и каталог магазина, книга не как просто таблица, а как объект каталога.

Данные могут быть разделены по бизнес-доменам, User domain, Catalog domain, Store domain.

User domain (физически отдельная DB) будет шарится по user_id, в нем хранятся:
- user
- user_profile
- user_setting
- user_session

Catalog domain + Store domain (логически два разных домена, физически живут в одной DB), шарятся по store_id
- StoreShard_1
- StoreShard_2
- StoreShard_3

Каждый shard хранит:
- магазин
- книги
- наличие на складе магазина
- цена

т.е. магазин это агрегат, а каталог - часть этого магазина.

В этом случае, книга является каталожной позицией магазина, а не просто строка в таблице. Каждая книга имеет одинаковые глобальные значения (Book_Metadata), такие как автор, название, год издания, издательство, ISBN.

Но в то же время у каждой книги есть переменные локальные значения, такие как цена в каждом магазине, наличие на складе магазина, количество на складе, продается по акции или нет.

Совокупность этой информации определяет книгу в конкретном магазине.

Поэтому логичнее разделить глобальные и локальные значения. Глобальные данные это как справочник, один на всю систему. Достаточно постоянный, меняется редко, относительно назначительный по объему. Локальные данные, это каталог конкретного магазина. Они изменчивы, к ним часто обращаются и логически они уникальны для каждого магазина.

Если все данные книги считать глобальными, то таблица `book` будет огромной, все магазины будут стучаться в одну DB, возможны множества конфликтов, система в целом будет сложнее масштабироваться.

Если книга часть магазина, то каждый магазин живет в своем shard, нагрузка распределяется, данные используются локально.

В целом, схема системы может выглядеть так:

```
                --------------
                │    API     │
                ------|-------
                      │
        --------------|-  ------------
        │        Backend             │
        │ (domain-aware routing)     │----------|
        --------│-----------│---------          |
                │           │                   |
     -----------│----  -----│------------  -----│----------
     | USER DOMAIN  |  |STORE + CATALOG |  | METADATA     |
     | (by user_id) |  | (by store_id)  |  |              |
      ----------|----  ----|-------------  ------|---------
     -----------│---- -----│-----------    ------│---------
     │ User_Shards A│ │ Store_Shards A│    │ Book_Metadata│
     │ Primary (RW) │ │ Primary (RW)  │    │ Primary (RW) │
     │ Replica (R)  │ │ Replica (R)   │    │ Replica (R)  │
     │ HotStandby   │ │ HotStandby    │    │ HotStandby   │
     │ Archive      │ │ Archive       │    │ Archive      │
     ----------|----- ----|------------    ----------------
               │          │
     ----------│----- ----│------------
     │ User Shards B│ │ Store Shards B│
     │ Primary (RW) │ │ Primary (RW)  │
     │ Replica (R)  │ │ Replica (R)   │
     │ HotStandby   │ │ HotStandby    │
     │ Archive      │ │ Archive       │
     ---------------- -----------------
```

Режимы работы серверов:

- Shared primary - чтение / запись
- Replica - чтение
- Hot standby - failover
- Archive - холодные данные

По хорошему, это должно быть совместным решением группы товарищей, а именно архитектора (как владельца системы с ясным взглядом в будущее), бэкенд разработчика (как реализатора решения) и системного администратора (как ответственного за работоспособность и устойчивость системы). 
---

